{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.13\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from warnings import warn\n",
    "\n",
    "import souphelper \n",
    "from souphelper import *\n",
    "\n",
    "import episodesEx \n",
    "from episodesEx import *\n",
    "\n",
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Episodes Scraping from Fandom \n",
    "\n",
    "We want scraping from Fandom site all the simpsons episodes with: \n",
    "\n",
    "* URL episode [string]\n",
    "* Number total [Integer]\n",
    "* Season number [Integer]\n",
    "* Title [String]\n",
    "* Air date [Date with no time]\n",
    "* Production code [string]\n",
    "* Main characters [list of strings]\n",
    "* Written by [String]\n",
    "* Directed by [String]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://simpsons.fandom.com\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a series of functions for the attributes, to which we are interested, of the episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "STR_SEPARATOR = \",\"\n",
    "\n",
    "def title(episodeInfobox:bs):\n",
    "    if episodeInfobox:\n",
    "        titleTag = episodeInfobox.find(\"h2\")\n",
    "        if titleTag:\n",
    "            return str(titleTag.string).strip()\n",
    "    return None\n",
    "\n",
    "def image(episodeInfobox:bs):\n",
    "    if episodeInfobox:\n",
    "        imageTag = episodeInfobox.figure\n",
    "        if imageTag:\n",
    "            return imageTag.a.img[\"src\"]\n",
    "    return None\n",
    "\n",
    "def episode_number(episodeInfobox:bs):\n",
    "    if episodeInfobox:\n",
    "        episodeNumberTag = episodeInfobox.find(attrs={\"data-source\": \"Episode Number\"})\n",
    "        if episodeNumberTag:\n",
    "            return str(episodeNumberTag.div.string).strip()\n",
    "    return None\n",
    "\n",
    "def season(episodePage):\n",
    "    if episodePage:\n",
    "        number_season = episodePage.find(attrs={\"data-tracking-label\": \"categories-top-more-0\"}).string.strip()\n",
    "        res = [int(i) for i in number_season.split() if i.isdigit()]\n",
    "        if len(res) == 1:\n",
    "            return res[0]\n",
    "        elif len(res) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            warn(\"More than one season number forund\" + str(res))\n",
    "            return res[0]\n",
    "        return res\n",
    "    return None\n",
    "\n",
    "def production_code(episodeInfobox:bs):\n",
    "    if episodeInfobox:\n",
    "        production_codeTag = episodeInfobox.find(attrs={\"data-source\": \"productionCode\"})\n",
    "        if production_codeTag:\n",
    "            return str(production_codeTag.div.string).strip()\n",
    "    return None\n",
    "\n",
    "def airdate(episodeInfobox:bs):\n",
    "    if episodeInfobox:\n",
    "        airdateTag = episodeInfobox.find(attrs={\"data-source\": \"originalAirdate\"})\n",
    "        if airdateTag:\n",
    "            return str(airdateTag.div.string).strip()\n",
    "    return None\n",
    "\n",
    "def maincharacters(episodeInfobox:bs):\n",
    "    if episodeInfobox:\n",
    "        mainCharactersTag = episodeInfobox.find(attrs={\"data-source\": \"main_character(s)\"})\n",
    "        if mainCharactersTag:\n",
    "            mainCharactersContent = mainCharactersTag.div\n",
    "            handleP(mainCharactersContent)\n",
    "            handleLinks(mainCharactersContent)\n",
    "            handleLinebreaks(mainCharactersContent, STR_SEPARATOR)\n",
    "            return str(mainCharactersContent.string).strip()\n",
    "    return None\n",
    "\n",
    "def writtenby(episodeInfobox:bs):\n",
    "    if episodeInfobox:\n",
    "        writtenbyTag = episodeInfobox.find(attrs={\"data-source\": \"Written By\"})\n",
    "        if writtenbyTag:\n",
    "            writtenbyContent = writtenbyTag.div\n",
    "            handleP(writtenbyContent)\n",
    "            handleLinks(writtenbyContent)\n",
    "            handleLinebreaks(writtenbyContent, STR_SEPARATOR)\n",
    "            return str(writtenbyTag.div.string).strip()\n",
    "    return None\n",
    "\n",
    "def directedby(episodeInfobox:bs):\n",
    "    if episodeInfobox:\n",
    "        directedbyTag = episodeInfobox.find(attrs={\"data-source\": \"Directed By\"})\n",
    "        if directedbyTag:\n",
    "            directedbyContent =directedbyTag.div\n",
    "            handleP(directedbyContent)\n",
    "            handleLinks(directedbyContent)\n",
    "            handleLinebreaks(directedbyContent, STR_SEPARATOR)\n",
    "            return str(directedbyTag.div.string).strip()\n",
    "    return None\n",
    "\n",
    "def episodeAttrs(episodePage:bs, **moreAttributes):\n",
    "    infobox = episodePage.find(class_=\"portable-infobox\")\n",
    "    return {\n",
    "        **moreAttributes,\n",
    "        \"title\": episodePage.find(id=\"firstHeading\").string.strip(),\n",
    "        \"image_url\": image(infobox),\n",
    "        \"season\": season(episodePage),\n",
    "        \"episode_number_absolute\": episode_number(infobox),\n",
    "        \"production_code\": production_code(infobox),\n",
    "        \"airdate\": airdate(infobox),\n",
    "        \"main_characters\": maincharacters(infobox),\n",
    "        \"written_by\": writtenby(infobox),\n",
    "        \"directed_by\": directedby(infobox)\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_PAGE = \"https://simpsons.fandom.com/wiki/List_of_Episodes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esepisodesPage(page:bs):\n",
    "    return page.find(id=\"firstHeading\").string.strip() == \"List of Episodes\"\n",
    "\n",
    "def episodesURLs(episodesPage:bs):\n",
    "    if not esepisodesPage(episodesPage):\n",
    "        raise ValueError(\"Soup received is not a episodes page\")\n",
    "    \n",
    "    episodes = episodesPage.find_all(\"td\", class_=\"oLeft\")\n",
    "    \n",
    "    links = []\n",
    "    for episode in episodes:\n",
    "        links.append(BASE_URL + episode.b.a[\"href\"])\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeEpisode(url:str):\n",
    "    if url in exceptions:\n",
    "        return None\n",
    "    episodePage = soup(url)\n",
    "    episode = episodeAttrs(episodePage, url=url)\n",
    "    return episode\n",
    "\n",
    "def scrapeEpisodesPage(url:str):\n",
    "    episodes = []\n",
    "    episodesPage = soup(url)\n",
    "\n",
    "    for episodeURL in episodesURLs(episodesPage):\n",
    "        episode = scrapeEpisode(episodeURL)\n",
    "        if episode:\n",
    "            episodes.append(episode)\n",
    "    return episodes\n",
    "\n",
    "def scrapeEpisodes(EpisodePage = EPISODE_PAGE):\n",
    "    episodes = []\n",
    "    episodesPageURL = EpisodePage\n",
    "    \n",
    "    pageEpisodes = scrapeEpisodesPage(episodesPageURL)\n",
    "    episodes.extend(pageEpisodes)\n",
    "\n",
    "    return episodes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to write csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_FILE_NAME = \"episodes\"\n",
    "CSV_FILE_EXTENSION = \"csv\"\n",
    "\n",
    "def writeCsv(data:list[dict], filename = CSV_FILE_NAME):\n",
    "    if not data or len(data) <= 0:\n",
    "        return\n",
    "        \n",
    "    with open(filename+\".\"+CSV_FILE_EXTENSION, 'w', encoding='utf-8' , newline='') as f:\n",
    "        writer = csv.DictWriter(f,\n",
    "            fieldnames=data[0].keys(),\n",
    "            delimiter=';',\n",
    "            quotechar='\"',\n",
    "            escapechar=\"\\\\\",\n",
    "            quoting=csv.QUOTE_NONNUMERIC\n",
    "        )\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping a single episode (if we insert link in EPISODE_TEST_URL variable) or all episodes (otherwise). In the second case we have the time spent for scraping, the time for export and the total time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed in 7 min (430.2437951564789 sec)\n",
      "Now exporting to episodes\n",
      "Export completed in 0 min (0.00896310806274414 sec)\n",
      "Number of episodes scraped: 753\n",
      "Finished in 7 min (430.25478529930115 sec)\n"
     ]
    }
   ],
   "source": [
    "EPISODE_TEST_URL = None # scrape only this if not None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if EPISODE_TEST_URL:\n",
    "        print(\"Testing single episode \" + EPISODE_TEST_URL)\n",
    "        pprint(scrapeEpisode(EPISODE_TEST_URL), sort_dicts=False)\n",
    "    else:\n",
    "        # Scrape\n",
    "        startScrapingTime = time.time()\n",
    "        episodes = scrapeEpisodes()\n",
    "        scrapingTime = time.time() - startScrapingTime\n",
    "        print(\"Scraping completed in \" + str(int(scrapingTime/60)) + \" min (\" + str(scrapingTime) + \" sec)\")\n",
    "        \n",
    "        # Export\n",
    "        startExportTime = time.time()\n",
    "        print(\"Now exporting to \" + CSV_FILE_NAME)\n",
    "        writeCsv(episodes)\n",
    "        exportTime = time.time() - startExportTime\n",
    "        print(\"Export completed in \" + str(int(exportTime/60)) + \" min (\" + str(exportTime) + \" sec)\")\n",
    "        \n",
    "        # Total\n",
    "        totalTime = time.time() - startScrapingTime\n",
    "        print(\"Number of episodes scraped: \" + str(len(episodes)))\n",
    "        print(\"Finished in \" + str(int(totalTime/60)) + \" min (\" + str(totalTime) + \" sec)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18b0eca292df7790ae02c99d513840323a4ed6febc80d490ebe22b975c5e1f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
